{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21e820b-74b2-43be-adce-b92dfa2df7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\veena\\Downloads\\bomberman_rl-master')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b507fcf-8fc3-4390-99ae-376a2eeaf8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import BombeRLeWorld, WorldArgs\n",
    "import events as e \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa73e2f-3fa5-49b2-8bf0-e7f0e89b52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import WorldArgs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4f4270-ef48-48cf-961c-74592977a685",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BombeRLeWorld' object has no attribute 'rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m agents \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Add your agents here\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize the environment with the specified arguments and agents\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m env \u001b[38;5;241m=\u001b[39m BombeRLeWorld(args, agents)\n",
      "File \u001b[1;32m~\\environment.py:419\u001b[0m, in \u001b[0;36mBombeRLeWorld.__init__\u001b[1;34m(self, args, agents)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m agents\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbombs \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Example: a list to keep track of bombs\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marena \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_arena()\n",
      "File \u001b[1;32m~\\environment.py:376\u001b[0m, in \u001b[0;36mBombeRLeWorld.build_arena\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    373\u001b[0m scenario_info \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mSCENARIOS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mscenario]\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# Crates in random locations\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m arena[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mrandom((s\u001b[38;5;241m.\u001b[39mCOLS, s\u001b[38;5;241m.\u001b[39mROWS)) \u001b[38;5;241m<\u001b[39m scenario_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRATE_DENSITY\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m CRATE\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Walls\u001b[39;00m\n\u001b[0;32m    379\u001b[0m arena[:\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;241m=\u001b[39m WALL\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BombeRLeWorld' object has no attribute 'rng'"
     ]
    }
   ],
   "source": [
    "args = WorldArgs(\n",
    "    no_gui=True,                # Set True to run without GUI\n",
    "    fps=30,                     # Frames per second\n",
    "    turn_based=False,            # Whether the game is turn-based\n",
    "    update_interval=0.1,         # Interval between updates in seconds\n",
    "    save_replay=False,           # Whether to save a replay of the game\n",
    "    replay=None,                 # Replay file (if any)\n",
    "    make_video=False,            # Whether to record the game as a video\n",
    "    continue_without_training=False, # Continue the game without training\n",
    "    log_dir=\"C:/Users/veena/Downloads/bomberman_rl-master/bomberman_rl-master/logs\",  # Set the log directory\n",
    "    save_stats=False,            # Whether to save game statistics\n",
    "    match_name=\"Test Match\",     # Name of the match\n",
    "    seed=42,                     # Random seed for reproducibility\n",
    "    silence_errors=False,        # Whether to silence errors\n",
    "    scenario=\"treasure-hunt\"              # Scenario for the game\n",
    ")\n",
    "\n",
    "# Create a list of agents (could be AI agents or rule-based)\n",
    "agents = []  # Add your agents here\n",
    "\n",
    "# Initialize the environment with the specified arguments and agents\n",
    "env = BombeRLeWorld(args, agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f0eb08-bf85-4935-bf2a-2856e6f759cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"C:/Users/veena/Downloads/bomberman_rl-master/bomberman_rl-master/logs\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bfa69-1fa7-40e8-b368-7468a12cddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        self.q_table = {}  # This will store state-action pairs and their Q-values\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def state_to_tuple(self, state):\n",
    "        \"\"\"Convert the game state dictionary into a hashable tuple, including handling numpy arrays and lists.\"\"\"\n",
    "        def convert_value(value):\n",
    "            # Convert numpy arrays to tuples\n",
    "            if isinstance(value, np.ndarray):\n",
    "                return tuple(value.flatten())  # Flatten to 1D and convert to tuple\n",
    "            # Convert lists to tuples\n",
    "            elif isinstance(value, list):\n",
    "                return tuple(convert_value(v) for v in value)  # Recursively convert list elements\n",
    "            # Convert nested dictionaries\n",
    "            elif isinstance(value, dict):\n",
    "                return self.state_to_tuple(value)  # Recursively convert dictionaries\n",
    "            return value\n",
    "\n",
    "        # Convert all state values to hashable types (like tuples)\n",
    "        return tuple((k, convert_value(v)) for k, v in state.items())\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"Choose the best action based on current Q-values\"\"\"\n",
    "        # Convert the state to a hashable type\n",
    "        state_tuple = self.state_to_tuple(state)\n",
    "\n",
    "        if state_tuple not in self.q_table:\n",
    "            self.q_table[state_tuple] = {action: 0 for action in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "\n",
    "        # Epsilon-greedy action selection (explore vs exploit)\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            # Explore: Take a random action\n",
    "            return np.random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT'])\n",
    "        else:\n",
    "            # Exploit: Take the action with the highest Q-value\n",
    "            return max(self.q_table[state_tuple], key=self.q_table[state_tuple].get)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value based on the received reward\"\"\"\n",
    "        # Convert the states to hashable types\n",
    "        state_tuple = self.state_to_tuple(state)\n",
    "        next_state_tuple = self.state_to_tuple(next_state)\n",
    "\n",
    "        if state_tuple not in self.q_table:\n",
    "            self.q_table[state_tuple] = {a: 0 for a in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "        if next_state_tuple not in self.q_table:\n",
    "            self.q_table[next_state_tuple] = {a: 0 for a in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "\n",
    "        # Q-Learning update rule\n",
    "        best_next_action = max(self.q_table[next_state_tuple], key=self.q_table[next_state_tuple].get)\n",
    "        current_q = self.q_table[state_tuple][action]\n",
    "        max_future_q = self.q_table[next_state_tuple][best_next_action]\n",
    "\n",
    "        # Update Q-value\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q)\n",
    "        self.q_table[state_tuple][action] = new_q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb36ff-fc01-4049-8d56-678dd09e8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c7911-4c97-4f32-997c-2b92c54bf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm correctly\n",
    "\n",
    "def train_agent(agent, env, num_episodes=1000):\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        game_state = env.reset()  # Reset the environment at the start of each episode\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # The agent chooses an action based on the current game state\n",
    "            action = agent.predict(game_state)\n",
    "            \n",
    "            # Perform the action in the environment\n",
    "            new_game_state, reward, done, info = env.step(action)  # Using the new step method\n",
    "            \n",
    "            # Update the agent's Q-values based on the game state and reward\n",
    "            agent.update(game_state, action, reward, new_game_state)\n",
    "            \n",
    "            # Move to the next state\n",
    "            game_state = new_game_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc86a24-bff8-4d79-9f7e-dd7416581389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# Train the agent in the environment\n",
    "train_agent(agent, env, num_episodes=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c36205-a6e3-4aff-9d06-78062882c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4e6d4-d4e1-40b4-807e-c1fbf5eb2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        self.q_table = {}  # Q-table for state-action pairs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "    def predict(self, state):\n",
    "        \"\"\"Choose the best action based on current Q-values\"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {action: 0 for action in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "        \n",
    "        # Epsilon-greedy action selection (explore vs exploit)\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            # Explore: Take a random action\n",
    "            return np.random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT'])\n",
    "        else:\n",
    "            # Exploit: Take the action with the highest Q-value\n",
    "            return max(self.q_table[state], key=self.q_table[state].get)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value based on the received reward\"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {a: 0 for a in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = {a: 0 for a in ['UP', 'DOWN', 'LEFT', 'RIGHT', 'BOMB', 'WAIT']}\n",
    "        \n",
    "        # Q-Learning update rule\n",
    "        best_next_action = max(self.q_table[next_state], key=self.q_table[next_state].get)\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_future_q = self.q_table[next_state][best_next_action]\n",
    "        \n",
    "        # Update Q-value\n",
    "        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_future_q - current_q)\n",
    "        self.q_table[state][action] = new_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4594bbee-5b3a-4370-82d6-25442d862740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(game_state):\n",
    "    \"\"\"Extract features such as the agent's position, nearby coins, and bombs\"\"\"\n",
    "    # The agent's current position (x, y)\n",
    "    _, _, _, (x, y) = game_state['self']\n",
    "    \n",
    "    # List of coins on the board\n",
    "    coins = game_state['coins']\n",
    "    \n",
    "    # Calculate distance to the closest coin\n",
    "    coin_distances = [np.linalg.norm(np.array([x, y]) - np.array(coin)) for coin in coins]\n",
    "    closest_coin_distance = min(coin_distances) if coin_distances else float('inf')\n",
    "    \n",
    "    return (x, y, closest_coin_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1bdc0d-f554-404e-97f4-e39918bc41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(agent, game_state):\n",
    "    \"\"\"Make the agent choose an action based on the game state\"\"\"\n",
    "    features = extract_features(game_state)\n",
    "    action = agent.predict(features)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311548e-306e-4fa7-8230-f162d2a63d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_episodes=1000):\n",
    "    for episode in tqdm.tqdm(range(num_episodes)):\n",
    "        game_state = env.reset()  # Start a new game\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # The agent chooses an action based on the current game state\n",
    "            action = act(agent, game_state)\n",
    "            \n",
    "            # Perform the action in the environment\n",
    "            new_game_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update the Q-learning model based on the reward and the new state\n",
    "            features = extract_features(game_state)\n",
    "            new_features = extract_features(new_game_state)\n",
    "            agent.update(features, action, reward, new_features)\n",
    "            \n",
    "            # Move to the next state\n",
    "            game_state = new_game_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442ad05-829c-4b7e-9ef3-2f97d97052bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
